\section{Real Models and Real Problems: Feature Colinearity}

\subsection{Introduction: "Real" vs Binary Model Features}

We've worked through the matrix algebra in an abstract mathematical setting to
show the fundamental equation that drives ordinary least squares regression:

\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

It might be tempting at this point to accept this equation and move on,
considering it just a tool for future use.  We propose another perspective: that
this equation is a \emph{starting} point for building our intuitions and
deepening our understanding; that it's a distilled essence that has much to
teach us. \\

It will be easier to dive into this equation if we consider a new type of
model feature (or x-variable): binary-valued features.  So far, we've worked
exclusively with x-variables that can take on arbitrary floating point (real)
values.  Such variables are important and have wide application, but equally
important are binary features--x-variables that are allowed to take on only two
values, 0 or 1.\\

At first, binary features may seem constritive compared to real features, but in
fact they can allow our models to become quite a bit \emph{more} flexible.
Consider the following (fictional) example of algae mass as a function of
concentration of the algae-unfriendsly chemical X.  As the concentration rises,
algae mass decreases to zero, but the rate of decrease slows as the
concentration increases, and of course the algae mass cannot be negative.  A
linear fit using the real value of the concentration fails on both of these
counts, and would clearly not generalize well to high concentrations.\\

Another option is to "bin" concentration values (less than 1, 1-2, 2-5, 5-10,
more than 10), and establish binary variables corresponding to each bin.  So
instead of having a single decimal number associated with it, each data point
will have now 4 binary numbers associated with it (indicating whether or not the
concentration is within each of the bins).  Of course for any data point, only a
single bin membership variable will take the value 1, while the rest will be
zero. \\

PLOT COLIN-1\\

This kind of binary variable representation therefore allows us to use a linear
model framework to capture nonlinearities in the real variable.  Nothing is for
free though, and the cost of this flexibility becomes at model training time.
For our new 4-feature binary model, we now have to learn 4 coefficients, whereas
in the single floating point model, we had to learn 2 (one for the concentration
coefficient and one for the bias).  So the utility of this kind of strategy
depends strongly on how much data is available.  \\

Though binning real valued features is a nice trick, it is by no means the only
way binary variables arise.  Almost any membership description (e.g. any
demographic membership) is naturally featurized as a binary variable.  So, if
you'll be doing modeling in any real context, you should expect to be familiar.
As an added bonus, binary valued variables will simplify the action of the
regression equation (CONSISTENT NAME NEEDED HERE).  So, rest of this chapter,
we'll develop another fictitious example to help us explore 
\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

\subsection{Extended Example: Income by City}

Suppose we wanted to build a model to help us predict income based on a person's
city of residence.  



