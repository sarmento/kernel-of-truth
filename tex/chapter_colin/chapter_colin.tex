\section{Real Models and Real Problems: Feature Colinearity}

\subsection{Introduction: "Real" vs Binary Model Features}

We've worked through the matrix algebra in an abstract mathematical setting to
show the fundamental equation that drives ordinary least squares regression:

\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

It might be tempting at this point to accept this equation and move on,
considering it just a tool for future use.  We propose another perspective: that
this equation is a \emph{starting} point for building our intuitions and
deepening our understanding; that it's a distilled essence that has much to
teach us. \\

It will be easier to dive into this equation if we consider a new type of
model feature (or x-variable): binary-valued features.  So far, we've worked
exclusively with x-variables that can take on arbitrary floating point (real)
values.  Such variables are important and have wide application, but equally
important are binary features--x-variables that are allowed to take on only two
values, 0 or 1.\\

At first, binary features may seem constritive compared to real features, but in
fact they can allow our models to become quite a bit \emph{more} flexible.
Consider the following (fictional) example of algae mass as a function of
concentration of the algae-unfriendsly chemical X.  As the concentration rises,
algae mass decreases to zero, but the rate of decrease slows as the
concentration increases, and of course the algae mass cannot be negative.  A
linear fit using the real value of the concentration fails on both of these
counts, and would clearly not generalize well to high concentrations.\\

Another option is to "bin" concentration values (less than 1, 1-2, 2-5, 5-10,
more than 10), and establish binary variables corresponding to each bin.  So
instead of having a single decimal number associated with it, each data point
will have now 4 binary numbers associated with it (indicating whether or not the
concentration is within each of the bins).  Of course for any data point, only a
single bin membership variable will take the value 1, while the rest will be
zero. \\

PLOT COLIN-1\\

This kind of binary variable representation therefore allows us to use a linear
model framework to capture nonlinearities in the real variable.  Nothing is for
free though, and the cost of this flexibility becomes at model training time.
For our new 4-feature binary model, we now have to learn 4 coefficients, whereas
in the single floating point model, we had to learn 2 (one for the concentration
coefficient and one for the bias).  So the utility of this kind of strategy
depends strongly on how much data is available.  \\

Though binning real valued features is a nice trick, it is by no means the only
way binary variables arise.  Almost any membership description (e.g. any
demographic membership) is naturally featurized as a binary variable.  So, if
you'll be doing modeling in any real context, you should expect to be familiar.
As an added bonus, binary valued variables will simplify the action of the
regression equation (CONSISTENT NAME NEEDED HERE).  So, rest of this chapter,
we'll develop another fictitious example to help us explore 
\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

\subsection{Extended Example: Income by City}

Suppose we wanted to understand (and predict) how a person's city of residence
impacts their starting income in their first professional job. To simplify our
work, let's say we're interested in particular job category ("Data Scientist"),
and we're interested in the "residence" effect between the cities of Seattle and
Detroit.\\
\\
%\begin{center}
\begin{table}
\centering
\begin{tabular}{ | l | l | l |}
\hline
\multicolumn{3}{ |c| }{Data Scientist Starting Salaries} \\
\hline
Example Id  & City & Income \\ \hline
000 &  Seattle  & \$107,805 \\ \hline    
001 &  Seattle  & \$107,150 \\ \hline 
002 &  Detroit  & \$106,826 \\ \hline 
003 &  Detroit  & \$ 94,399 \\ \hline 
004 &  Detroit  & \$ 95,589 \\ \hline 
005 &  Seattle  & \$105,981 \\ \hline 
006 &  Detroit  & \$101,601 \\ \hline 
007 &  Seattle  & \$108,543 \\ \hline 
008 &  Seattle  & \$115,971 \\ \hline 
009 &  Detroit  & \$100,728 \\ \hline 

\end{tabular}
\end{table}
\\
\\
%\end{center}

Forgetting about regression for a moment, how might we handle this problem given
this data? A simple solution is to estimate the mean salary for each city and
compare.  Explicitly, this means we'd calculate:\\
\\
Detroit:
\begin{equation}
\label{naive_avg_1} 
\frac{\$106,826 + \$94,399 + \$95,589 + \$101,601 + \$100,728}{5} = \$99,828.60
\end{equation}
Seattle:
\begin{equation}
\label{naive_avg_2}
\frac{\$107,805 + \$107,150 + \$105,981 + \$108,543 + \$115,971}{5} = \$109,090.00
\end{equation}

Then we'd estimate the city-level effect to be
\begin{equation} 
\$109,090.00 - \$99,828.60 = \$9,261.40 
\end{equation}

Now let's take a look at how we would solve this problem in a linear regression
framework, and see how it compares to the simple averaging methodology.

First, we need to encode the city membership in terms of binary x-variables
(features) of the model.  There are two obvious ways to do this. \\
\\
1. Create two symmetric \emph{indicator} variables: one where '1' indicates to
Seattle residence, and a second where '1' indicates Detroit residence.\\
\\
2. Create a single city indicator and a "bias" variable.  The indicator variable
could be the above where '1' indicates Seattle residency and '0' indicates
Detroit.  The difference in the second (bias) variable, which takes the value
'1' for all data points, regardless of residency.\\
\\
Let's start with the first strategy.  In this case, our data is represted as
shown in table \ref{income_indicator_rep}\\

\begin{table}
\label{income_indicator_rep}
\centering
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Seattle\_indicator & Detroit\_indicator & Income \\ \hline
000 &  1  &  0  & \$107,805 \\ \hline    
001 &  1  &  0  & \$107,150 \\ \hline 
002 &  0  &  1  & \$106,826 \\ \hline 
003 &  0  &  1  & \$ 94,399 \\ \hline 
004 &  0  &  1  & \$ 95,589 \\ \hline 
005 &  1  &  0  & \$105,981 \\ \hline 
006 &  0  &  1  & \$101,601 \\ \hline 
007 &  1  &  0  & \$108,543 \\ \hline 
008 &  1  &  0  & \$115,971 \\ \hline 
009 &  0  &  1  & \$100,728 \\ \hline 

\end{tabular}
\caption{Income per person by city of residence data, represented in terms of symmetric
indicator variables (one for each city).}  
\end{table}

Now we can express the components in the OLS regression solution formula.  The
design matrix and target variable are:

\[
X = \left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
,
\quad\quad
Y = \left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\] 

Now let's break the OLS equation ($\hat{w} = (X^T X)^{-1} X^T Y $) down
piece-by-piece. \\
\\
First let's examine the product of the design matrix and the target.

\begin{align}
X^T Y &= 
\left(\begin{array}{cccccccccc}
         1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
         0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\\
X^T Y &= 
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\end{align}
\\
If you're noticing similarities to the naive averages in ~\ref{naive_avg_1} and
~\ref{naive_avg_2}, you're on the right track.\\

Now let's look at the product of the design matrix with itself:
\begin{align}
X^T X &=
\left(\begin{array}{cccccccccc}
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
          0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{cc}
          5 & 0 \\
          0 & 5 
\end{array}\right)
\end{align}
\\
Diagonal matrices are notoriously easy to invert, so the factor that explicitly
appears in the OLS equation is:
\begin{equation}
(X^T X)^{-1} =  
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\end{equation}
\\
Putting this all together gives:
\begin{align} 
\hat{w} &= (X^T X)^{-1} X^T Y 
\\
\hat{w} &= 
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\\
\\
\hat{w} &= 
\left(\begin{array}{c}
         \displaystyle\frac{107,805 + 107,150 + 105,981 + 108,543 + 115,971}{5} \\
         \\
         \displaystyle\frac{106,826 +  94,399 +  95,589 + 101,601 + 100,728}{5} 
\end{array}\right)
\\
\hat{w} &= 
\left(\begin{array}{c}
          109,090.00 \\
          99,828.60
\end{array}\right)
\end{align}
\\
The OLS calculation not only give the same answer as the naive averaging, it
encodes the exact logic of that calculation.  The calculation of $X^T Y$ simply
collects the measured incomes into the two groups based on residency and sums
them, while the calculation of $X^T X$ counts examples in each class, making
$(X^T X)^{-1}$ into the multipliers ($1/5$) to calculate the means. In this
case, we had equal sized groups, so the averaging process requires the same
divisor for both groups.  However, had the groups been of differing sizes, the
OLS equation would have gracefully adjusted the denominator as required by the
group size.\\
\\
%\\
%For those of you who may remember your early statistics classes, you may have
%seen the the problem of calculating observational means on different
%sub-populations handled by a technique calle ANOVA. The prototype ANOVA
%experiment would be one in which differing treatments ($t_i$) are given to
%sub-populations ($i$), and their relative effects on an output variable ($y$)
%are to be measured....NEEDS MORE\\
\\
When we developed our indicator-variable representation of table
\ref{income_indicator_rep} \{REFERENCE WRONG\}, we noted that there was an
alternative represention, using a single binary indicator variable
(Seattle-hood), and a "binary" bias variable, which was identically '1' for all
data examples.  We show this representation in table \ref{bias_residence_rep}. 

%\begin{table}
%\label{bias_residence_rep}
%\centering
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Bias & Residence & Income \\ \hline
000 &  1  &  1  & \$107,805 \\ \hline    
001 &  1  &  1  & \$107,150 \\ \hline 
002 &  1  &  0  & \$106,826 \\ \hline 
003 &  1  &  0  & \$ 94,399 \\ \hline 
004 &  1  &  0  & \$ 95,589 \\ \hline 
005 &  1  &  1  & \$105,981 \\ \hline 
006 &  1  &  0  & \$101,601 \\ \hline 
007 &  1  &  1  & \$108,543 \\ \hline 
008 &  1  &  1  & \$115,971 \\ \hline 
009 &  1  &  0  & \$100,728 \\ \hline 
\end{tabular}
\\
\\
%\caption{Income per person by city of residence data, represented in terms of
%bias and residence variables.}
%\end{table}
Let's take a closer look at the components of the OLS equation in this
represenation as well, starting wtih $X^T X$ 
\begin{equation}
X^T X =
\left(\begin{array}{cccccccccc}
          1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 
\end{array}\right)
\cdot
\left(\begin{array}{cc}
         1  &  1  \\ 
         1  &  1  \\ 
         1  &  0  \\ 
         1  &  0  \\ 
         1  &  0  \\ 
         1  &  1  \\ 
         1  &  0  \\ 
         1  &  1  \\ 
         1  &  1  \\ 
         1  &  0 
\end{array}\right)
=
\left(\begin{array}{cc}
          10 & 5 \\
          5  & 5 
\end{array}\right)
\end{equation}
\\
Unlike last time, where the diagonal terms counted the membership in each class,
here the diagonal encodes the number of 1's in the bias vector (10) and the
number of data points from the Seattle indicator class (5).  This time, let's
factor the matrix a bit.  There's nothing special about having 5 training
examples in each class, after all. Going forward we'll refer to the total number
of trianing examples as $N_e$.
\\
\begin{equation}
X^T X =
\left(\begin{array}{cc}
          10 & 5 \\
          5  & 5 
\end{array}\right)
= 5
\left(\begin{array}{cc}
          2 & 1 \\
          1 & 1 
\end{array}\right)
= \frac{N_e}{2}
\left(\begin{array}{cc}
          2 & 1 \\
          1 & 1 
\end{array}\right)
\end{equation}
\\
Giving:
\begin{equation}
(X^T X)^{-1} = \frac{2}{N_e}
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\end{equation}

We also have
\begin{align}
X^T Y &= 
\left(\begin{array}{cccccccccc}
         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
         1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\\
X^T Y &= 
\left(\begin{array}{c}
         107,805 + 107,150 + 106,826 + ... + 108,543 + 115,971 + 100,728 \\
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 
\end{array}\right)
\end{align}
The sum on top is over all data points, while the bottom is the sum over all
Seattle incomes.
We can factor this term similarly to $X^T X$ if we note that the average
over all incomes ($\bar{Y}$) is $\$104,459.3$, and the average over Seattle
incomes ($\bar{Y}_{Sea}$) is
$\$109,090.0$.
\begin{align}
X^T Y &= \frac{N_e}{2} 
\left(\begin{array}{c}
         2 \cdot 104,459.3 \\
         109,090.0 
\end{array}\right)
= \frac{N_e}{2} 
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\end{align}
Now let's calculate the model coefficients:
\begin{align}
\hat{w} = (X^T X)^{-1} X^T Y
&= \frac{2}{N_e}
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\cdot 
\frac{N_e}{2} 
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\\
&= 
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\\
&= 
\left(\begin{array}{c}
         2 \bar{Y} -   \bar{Y}_\mathit{Sea}  \\
        -2 \bar{Y} + 2 \bar{Y}_\mathit{Sea} 
\end{array}\right)
\end{align}
These terms may not look interpretable at first, but let's expand them out.
We'll put the incomes from Seattle residents in bold to help clarify what's
happening.
\begin{align}
\hat{w}_\mathit{bias} = 2 \bar{Y} -   \bar{Y}_{Sea} = \frac{1}{5}
  (&\mathbf{107,805} + \mathbf{107,150} + 106,826 + 94,399 + 95,589 \\
  &+ \mathbf{105,981} + 101,601 + \mathbf{108,543} + \mathbf{115,971} + 100,728)\\
- &\frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
\mathbf{108,543} + \mathbf{115,971})\\
= \frac{1}{5}(&106,826 + 94,399 + 95,589 + 101,601 + 100,728) 
= \$99,828.60
\end{align}
In calculating the model's bias coefficient, the matrix mathmatics is just
encoding an average over non-Seattle incomes (in agreement with our previous
results). This makes sense, because when we ask the model to predict an income,
we'll calculate:
\begin{equation}
y = \hat{w}_\mathit{bias} \cdot 1 + \hat{w}_\mathit{Res} \cdot x_\mathit{Res}
\end{equation}
Where $x_\mathit{Res}$ is 1 if the resident is from Seattle, 0 if not. Thie
means for a non-Seattle (Detroit) resident, the prediction is simply
\begin{equation}
y_\mathit{Det} = \hat{w}_\mathit{bias}
\end{equation}
The bias coefficient is the average of all Detroit residents because it is the
sole term in the income prediction for Detroit residents.\\
\\
Let's do the same thing to the second model coefficient, which we'll call
$\hat{w}_\mathit{Res}$. (Why not call this $\hat{w}_\mathit{Sea}$? Hang on and
you'll see.)
\begin{align}
\hat{w}_\mathit{Res} 
    &= -2 \bar{Y} + 2 \bar{Y}_\mathit{Sea} \\
    &= -\frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + 106,826 + 94,399 + 95,589 \\
    &\qquad + \mathbf{105,981} + 101,601 + \mathbf{108,543} + \mathbf{115,971} + 100,728)\\
    &\quad + \frac{2}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
                   \mathbf{108,543} + \mathbf{115,971})\\
    &= \frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
                   \mathbf{108,543} + \mathbf{115,971})\\
    &\quad -\frac{1}{5}(106,826 + 94,399 + 95,589 + 101,601 + 100,728) 
    &= \$9,261.39
\end{align}
The "residence" model coefficient is the difference between the average incomes
in the two cities (or, equivalently, the average difference in income).  This
makes sense in terms of model prediction, given the meaning of the bias
coefficient.  For a Seattle resident, the model predicts
\begin{align}
y &= \hat{w}_\mathit{bias} \cdot 1 + \hat{w}_\mathit{Res} \cdot x_\mathit{Res}\\
y &= \hat{w}_\mathit{bias} + \hat{w}_\mathit{Res}\\
y &= \$99,828.60 + \$9,261.39 = \$109,090.00 
\end{align}
With the indicator variable encoding, the model weights represented mean values
for each rewidency group, but for the bias/residency variable encoding, the
model weights represent a mean for one of the groups (Detroit) and a
differential effect size (the income gain one might expect when moving to
Seattle from Detroit). Note that this doesn't affect the outcome, both models
predict the same value for Seattle/Detroit resident incomes.  However, it's
now clear why we cautioned against naming the second model coefficient
$\hat{w}_\mathit{Sea}$.  This name could mislead us into thinking the model
weight is the mean of the Seattle group, when in fact it's a measure of the
emph{difference} between the groups. \\
\\
It's worth asking what happens if we try to combine these two representations.
What if we have a bias column \emph{and} the two indicators? This would mean the
model predictions would look like this:

\begin{align}
y &= \hat{w}_\mathit{bias} \cdot 1 
     + \hat{w}_\mathit{Sea} \cdot x_\mathit{Sea}
     + \hat{w}_\mathit{Det} \cdot x_\mathit{Det}\\
\end{align}

This looks pretty sensible, but what happens when we look at the design matrix?

\begin{align}
X^T X &=
\left(\begin{array}{cccccccccc}
          1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
          0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{ccc}
     1  &  1  &  0 \\ 
     1  &  1  &  0 \\ 
     1  &  0  &  1 \\ 
     1  &  0  &  1 \\ 
     1  &  0  &  1 \\ 
     1  &  1  &  0 \\ 
     1  &  0  &  1 \\ 
     1  &  1  &  0 \\ 
     1  &  1  &  0 \\ 
     1  &  0  &  1
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{ccc}
      10 & 5 & 5 \\
       5 & 5 & 0 \\
       5 & 0 & 5 \\
\end{array}\right)
\end{align}
\\
At first glance, this looks fine, but try to calculate $(X^T X)^{-1}$ and you'll
run into trouble. This matrix isn't invertible.  What's going on here? Why is
the system fragile in this way? Roughly speaking, the problem is that the we've
given the system \emph{too} many ways to make the prediction.  To fully
understand what's happening here, it will help if we can start with a working
system, and drive it closer and closer to breaking in this way.  Also, it will
help if we start to talk about the assumptions implicit in the linear modeling
framework.

\subsection{More features, more problems}

To get into this, let's return to our income prediction model, but this time,
let's try to understand how well our model estimates converge to the "truth" as
we sample more and more people. For that, of course, we need some "truth."  So,
let's imagine we're modeling a system where average Detroit income is \$100,000
per year, average Seattle income is \$110,000.  There will be some spread of
incomes around the average, so let's assume that's well described by a Gaussian
(normal) distribution, with scale (standard deviation) of \$10,000.\\
\\
FIGURE-COLIN-2\\
\\
This means the distribution of incomes looks something like FIGURE-COLIN-2.
The means in the two cities are fairly well separated, but there is definitely
overlap between the two distributions (meaning we should expect to see some
reasonable number of Detroit residents with higher income than some
Seattlites).\\
\\
We can write down the true equation we hope our model will discover.  We'll
stick to the bias/residence variable encoding.\\
\begin{align}
y &= w_\mathit{bias} \cdot 1 
    + w_\mathit{res} \cdot x_\mathit{res} \\
&\mathit{or} \\
y &= 100,000 \cdot 1 + 10,000 \cdot x_\mathit{res}
\end{align}
\\
Now for the final twist.  Let's say we (the researcher) collect another variable
for our model: favorite football team. Being open-minded, we're willing to
consider the possibility that someone's football team of choice may in fact be
driving income disparity, instead of (or in addition to) their residence.  Given
the "true" model, the addition of the new variable is clearly spurious.  To be
pedantic, we can of course explicitly include it in the formula.\\
\begin{align}
y &= w_\mathit{bias} \cdot 1 
    + w_\mathit{res} \cdot x_\mathit{res} 
    + w_\mathit{team} \cdot x_\mathit{team} 
    + \epsilon \\
&\mathit{or} \\
y &= 100,000 \cdot 1 + 10,000 \cdot x_\mathit{res} + 0 \cdot x_\mathit{team} +
\epsilon
\end{align}
\\
Where $x_\mathit{team}=1$ if the person in question is a Seattle Seahawks fan, and is
$0$ if the person is a Detroit Lions fan, and where we're using $\epsilon$ to
indicate the normally distributed error term.  Given enough data, we expect to be
able to detect that the model coefficient for the team is $0$, but intuitively,
we should be worried.  What if we never observe a Seattle resident who is also a
Lions fan, and/or we never observe a Detroit resident who is a Seahawks fan.  In
that case, there would be no way to tell if it's people's Seattle-ness or
Seahawk-loving-ness that's driving their higher income.  In fact, this
is precisely what happened to the "double-encoding" example at the end of the
last section. Let's dig into the linear algebra to flesh out that connection,
and let's start with a small but non-zero chance of finding the unlikely
fan/residence combinations.\\
\\
\begin{table}
\label{data_generation_matrix_I}
\centering
\begin{tabular}{l  l  l}
\hline
\multicolumn{3}{ c }{Data generation matrix I} \\
\hline
Residence &  Team     &  Probability \\ \hline
Seattle   &  Seahawks &  0.40 \\ \hline 
Seattle   &  Lions    &  0.10 \\ \hline 
Detroit   &  Seahawks &  0.45 \\ \hline 
Detroit   &  Lions    &  0.05 \\ \hline  
\end{tabular}
\caption{Probability matrix describing the data generation}  
\end{table}
\\
\begin{table}
\label{sample_data_I}
\centering
\begin{tabular}{l|  l | l | l || c} 
\hline
\multicolumn{5}{ c }{Sample Data I} \\
\hline
Example Id &  bias &  $x_\mathit{res}$ &  $x_\mathit{team}$ &  $y$       \\ \hline
000        &  1    &  0         &  0    &  \$78,949.27  \\ \hline  
001        &  1    &  0         &  0    &  \$92,532.56  \\ \hline 
002        &  1    &  1         &  1    & \$111,949.29  \\ \hline 
003        &  1    &  0         &  1    &  \$94,651.07  \\ \hline 
004        &  1    &  0         &  0    & \$107,132.24  \\ \hline 
...        &  ...  &  ...       &  ...  & ...           \\ \hline 
994        &  1    &  0         &  1    & \$102,143.79  \\ \hline 
995        &  1    &  1         &  1    & \$109,388.13  \\ \hline 
996        &  1    &  1         &  1    & \$102,288.42  \\ \hline 
997        &  1    &  1         &  0    & \$110,736.66  \\ \hline 
998        &  1    &  1         &  1    & \$118,216.87  \\ \hline 
999        &  1    &  1         &  1    & \$121,005.16  \\ \hline 
\end{tabular}
\caption{Example data from the probabilty matrix above}  
\end{table}
\\
The data under such conditions might look like that in table
\ref{sample_data_I}.  Let's look at what kind of $X^T X$ a typical sampling
might engender.
\begin{align}
X^T X &=
\left(\begin{array}{ccccccccccc}
          1 & 1 & 1 & 1 & 1 & ... & 1 & 1 & 1 & 1 & 1 \\
          0 & 0 & 1 & 0 & 0 & ... & 0 & 1 & 1 & 1 & 1 \\
          0 & 0 & 1 & 1 & 0 & ... & 1 & 1 & 0 & 1 & 1
\end{array}\right)
\cdot
\left(\begin{array}{ccc}
     1    &  0    &  0   \\ 
     1    &  0    &  0   \\  
     1    &  1    &  1   \\  
     1    &  0    &  1   \\  
     1    &  0    &  0   \\  
     ...  &  ...  &  ... \\  
     1    &  0    &  1   \\  
     1    &  1    &  1   \\  
     1    &  1    &  1   \\  
     1    &  1    &  0   \\  
     1    &  1    &  1   \\  
     1    &  1    &  1     
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{l}
          \mathbf{b} \\
          \mathbf{x}_\mathit{res} \\
          \mathbf{x}_\mathit{team} 
\end{array}\right)
\cdot
\left(\begin{array}{ccc}
     \mathbf{b} & \mathbf{x}_\mathit{res} & \mathbf{x}_\mathit{team}  
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{lll}
          \mathbf{b} \cdot \mathbf{b} 
              & \mathbf{b} \cdot \mathbf{x}_\mathit{res} 
              & \mathbf{b} \cdot \mathbf{x}_\mathit{team}   
              \\
          \mathbf{x}_\mathit{res} \cdot \mathbf{b}  
              & \mathbf{x}_\mathit{res} \cdot \mathbf{x}_\mathit{res} 
              & \mathbf{x}_\mathit{res} \cdot \mathbf{x}_\mathit{team}   
              \\
          \mathbf{x}_\mathit{team} \cdot \mathbf{b}  
              & \mathbf{x}_\mathit{team} \cdot \mathbf{x}_\mathit{res}     
              & \mathbf{x}_\mathit{team} \cdot \mathbf{x}_\mathit{team}    
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{ccc}
          1000 & 507 & 558 \\
           507 & 507 & 461 \\
           558 & 461 & 558
\end{array}\right)
\end{align}
Where we've introduced vector notation to help us understand each of the entries
in the final matrix.  This notation helps illustrate the main point, that for
binary varibles, $X^T X$ is always just a counts matrix.  The upper left $(0,0)$
entry is the inner product of the bias vector with itself, which simply counts
the total number of data points.  The $(1,1)$ entry is a count of seattle
residents (recall $x_{res}=1$ indicates Seattle residency), and the $(2,2)$
entry counts the number of Seahawks fans.  The off-diagonal entries are
co-occurance counts.  For example the $(0,1)$ entry is the co-occurance of
Seattle residency and the bias, which is again just a count of Seattle residents
(since the bias is a vector of 1's).  The $(1,0)$ entry is identical, and the
$(2,0)$ and $(0,2)$ entries are co-occurance counts of Seahawks fans with the
bias, which again degenerates to just counting Seahawks fans.  The remaining
entries $(1,2)$ and $(2,1)$ are city/team co-occurance counts, showing that of
the 507 Seattle residents, only 461 are Seahawks fans.\\
\\
Let's calculate the model coefficients using $\hat{w} = (X^T X)^{-1} X^T Y$:
\begin{align}
  \hat{w}_\mathit{bias} &= 99735.33 \\
  \hat{w}_\mathit{res}  &= 9721.63 \\
  \hat{w}_\mathit{team} &= 624.16
\end{align}
This model did well.  We're only attributing \$624 to the football preference, and
the bias and residence effect are very close to the true values.  However, we
started with a pretty lenient assumption about crossover fans.  What if we
aren't so generous.  
\\
\begin{table}
\label{data_generation_matrix_II}
\centering
\begin{tabular}{l  l  l}
\hline
\multicolumn{3}{ c }{Data generation matrix I} \\
\hline
Residence &  Team     &  Probability \\ \hline
Seattle   &  Seahawks &  0.490 \\ \hline 
Seattle   &  Lions    &  0.010 \\ \hline 
Detroit   &  Seahawks &  0.495 \\ \hline 
Detroit   &  Lions    &  0.005 \\ \hline  
\end{tabular}
\caption{Probability matrix describing the data generation}  
\end{table}
\\
Here are a few outcomes:\\
\begin{align}
  \hat{w}_\mathit{bias} &= 100184.83, 100020.99, 99812.83 \\
  \hat{w}_\mathit{res}  &= 6450.88, 9813.06, 12833.04 \\
  \hat{w}_\mathit{team} &= 2884.30, 455.14, -3083.80
\end{align}
The model is still attributing more importance to the city variable than to
football team, but the variance in the team coefficient is very high now.
(Note that the bias term is highly stable and unaffected by our city vs. team
drama.)  To quantify this, we can run many models and calculate the mean and standard
deviation of the coefficients.  We get:\\
\begin{align}
  \hat{w}_\mathit{bias} &= 100013.05 \pm 435.33 \\
  \hat{w}_\mathit{res}  &= 9490.81 \pm 2754.02 \\
  \hat{w}_\mathit{team} &= 552.82 \pm 2803.31
\end{align}
Intuitively, we know we have a problem here based on the data generation, but
how can we quantify and identify it systematically?  The root cause of this
problem is that we are lacking training data examples with a certain combination
of variables, meaning the issue really doesn't depend on our depdendent variable
(income) at all.  Let's look closely at a version of $X^T X$ in the case where
we have a very low likelihood of "crossover" fans.

\begin{align}
X^T X &=
\left(\begin{array}{ccc}
          1000 & 507 & 504 \\
           507 & 507 & 498 \\
           504 & 498 & 504
\end{array}\right)
\\
X^T X &=
500 
\left(\begin{array}{ccc}
           2     & 1.014 & 1.008 \\
           1.014 & 1.014 & 0.996 \\
           1.008 & 0.996 & 1.008
\end{array}\right)
\end{align}
Here's the key point: the last two columns of the matrix are very nearly equal
(because derived fact that almost everyone in Seattle is a Seahawks fan and
almost everyone in Detroit is a Lions fan).  We say the last two column vectors
are nearly "colinear," and this has serious consequences when we try to invert
this matrix.  Let's consider 
\begin{align}
(X^T X)^{-1} &=
\frac{1}{500}
\left(\begin{array}{ccc}
           2     & 1.014 & 1.008 \\
           1.014 & 1.014 & 0.996 \\
           1.008 & 0.996 & 1.008
\end{array}\right)^{-1}
=
\frac{1}{500}
\left(\begin{array}{ccc}
         1.0192  &  -0.6144 &  -0.4120 \\
        -0.614 4 &  33.8632 & -32.8456 \\
        -0.4120  & -32.8456 &  33.8587 
\end{array}\right)^{-1}
\end{align}
Let's drive the matrix closer to fully colinear and see what happens to the
inverse.  We won't change any single elment by more than 2\%.
\begin{align}
\left(\begin{array}{ccc}
           2     & 1.001 & 1.001 \\
           1.001 & 1.001 & 0.999 \\
           1.001 & 0.999 & 1.001
\end{array}\right)^{-1}
=
\left(\begin{array}{ccc}
          1.0025 &   -0.5139  &   -0.4901 \\
         -0.7338 &  244.3506  & -243.3473 \\
         -0.2696 & -243.5679  &  244.5697
\end{array}\right)^{-1}
\end{align}
The $\leq 2\%$ change in the matrix entries engenders a factor of 8 explosion in
the inverse in some entries. Worse, some of these terms are very large and of
opposite sign.  This creates the real problem, which we can see by looking at
$X^T Y$.

\begin{align}
X^T Y = 
\left(\begin{array}{l}
          \mathbf{b} \\
          \mathbf{x}_\mathit{res} \\
          \mathbf{x}_\mathit{team} 
\end{array}\right)
\cdot
\left(\begin{array}{c}
          \mathbf{y}
\end{array}\right)
=
\left(\begin{array}{r}
          \mathbf{b} \cdot \mathbf{y} \\
          \mathbf{x}_\mathit{res} \cdot \mathbf{y} \\
          \mathbf{x}_\mathit{team} \cdot \mathbf{y}  
\end{array}\right)
\end{align}
Remember, $\mathbf{y}$ is just our vector of incomes, so $\mathbf{b} \cdot
\mathbf{y}$ is the sum over all incomes, or 1000 times the average income
$\mathbf{\bar{y}}$  (if we have 1000 data points).  Similarly,
$\mathbf{x}_\mathit{res} \cdot \mathbf{y}$ is proportional to the average over
all Seattle residents' incomes ($\mathbf{\bar{y}}_\mathit{sea})$, and
$\mathbf{x}_\mathit{team} \cdot \mathbf{y}$ is proportional to the average over
all Seahawks fans' incomes ($\mathbf{\bar{y}}_\mathit{hawks}$).\\

Let's stick with considering we have 1000 data points, just to keep things more
concrete, and break down the final calculation of the model coefficients using
our blown up matrix above. To be illustrative, we'll approximate the number of
Seattle residents and the number of Seahawks fans as 500.
\begin{align}
\hat{w} = 
(X^T X)^{-1} X^T Y &=
\frac{1}{500}
\left(\begin{array}{ccc}
          1.0025 &   -0.5139  &   -0.4901 \\
         -0.7338 &  244.3506  & -243.3473 \\
         -0.2696 & -243.5679  &  244.5697
\end{array}\right)^{-1}
\cdot 
\left(\begin{array}{r}
          \mathbf{b} \cdot \mathbf{y} \\
          \mathbf{x}_\mathit{res} \cdot \mathbf{y} \\
          \mathbf{x}_\mathit{team} \cdot \mathbf{y}  
\end{array}\right)
\\
\hat{w} &= 
\frac{1}{500}
\left(\begin{array}{ccc}
          1.0025 &   -0.5139  &   -0.4901 \\
         -0.7338 &  244.3506  & -243.3473 \\
         -0.2696 & -243.5679  &  244.5697
\end{array}\right)^{-1}
\cdot 
500
\left(\begin{array}{l}
          2 \mathbf{\bar{y}} \\
          \mathbf{\bar{y}}_\mathit{sea} \\
          \mathbf{\bar{y}}_\mathit{hawks}
\end{array}\right)
\end{align}
Giving us, with a little rounding,
\begin{align}
\left(\begin{array}{l}
          \hat{w}_\mathit{bias} \\
          \hat{w}_\mathit{res} \\
          \hat{w}_\mathit{team} 
\end{array}\right)
&=
\left(\begin{array}{l}
          \quad 2 \cdot (1.00)  \, \mathbf{\bar{y}}
                 - \;\;\; 0.51 \,\, \mathbf{\bar{y}}_\mathit{sea} 
                 - \;\;\; 0.49 \,\, \mathbf{\bar{y}}_\mathit{hawks} \\
          \ 2 \cdot (-0.73) \,\, \mathbf{\bar{y}} 
              + 244.3  \,\, \mathbf{\bar{y}}_\mathit{sea} 
              - 243.3  \,\, \mathbf{\bar{y}}_\mathit{hawks} \\
          \ 2 \cdot (-0.27) \,\, \mathbf{\bar{y}} 
              - 243.6  \,\, \mathbf{\bar{y}}_\mathit{sea} 
              + 244.6  \,\, \mathbf{\bar{y}}_\mathit{hawks}
\end{array}\right)
\end{align}
Now that we have the equation in this form, we can understand it more clearly.
The bias coefficient is calculated more or less as you'd expect: from the total
sum over incomes, the seattle-dweller incomes are removed, as are the seahawks
fans, both in about equal parts.  This is much like we say before in the
bias/residence computation before we included team information.  This is also
why the bias is so robust aginst the rare crossover fan problem: if Seattle
residency and Seahawks fanhood were perfectly correlated, note that this
expression gives the same value no matter what the coefficients in front of
$\mathbf{\bar{y}}_\mathit{sea}$ and $\mathbf{\bar{y}}_\mathit{hawks}$ are, as long
as they sum to 1.  However, the story is different when we look the calculation
of both the residence effect and the team effect.  In both of these cases, the
Seattle income average and Seahawk fan income average are blown up (200x) and
subtracted from one another.  The OLS calculation must magnify whatever
difference there is on average between these two groups. This is reasonable,
since if the two groups highly overlap, these averages will be very close.  In
fact, if 1 out of about 250 Seattlites were not a seahawks fan, you'd expect to
have to scale up the difference between the two averages by 250 times.


