\section{Real Models and Real Problems: Feature Colinearity}

\subsection{Introduction: "Real" vs Binary Model Features}

We've worked through the matrix algebra in an abstract mathematical setting to
show the fundamental equation that drives ordinary least squares regression:

\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

It might be tempting at this point to accept this equation and move on,
considering it just a tool for future use.  We propose another perspective: that
this equation is a \emph{starting} point for building our intuitions and
deepening our understanding; that it's a distilled essence that has much to
teach us. \\

It will be easier to dive into this equation if we consider a new type of
model feature (or x-variable): binary-valued features.  So far, we've worked
exclusively with x-variables that can take on arbitrary floating point (real)
values.  Such variables are important and have wide application, but equally
important are binary features--x-variables that are allowed to take on only two
values, 0 or 1.\\

At first, binary features may seem constritive compared to real features, but in
fact they can allow our models to become quite a bit \emph{more} flexible.
Consider the following (fictional) example of algae mass as a function of
concentration of the algae-unfriendsly chemical X.  As the concentration rises,
algae mass decreases to zero, but the rate of decrease slows as the
concentration increases, and of course the algae mass cannot be negative.  A
linear fit using the real value of the concentration fails on both of these
counts, and would clearly not generalize well to high concentrations.\\

Another option is to "bin" concentration values (less than 1, 1-2, 2-5, 5-10,
more than 10), and establish binary variables corresponding to each bin.  So
instead of having a single decimal number associated with it, each data point
will have now 4 binary numbers associated with it (indicating whether or not the
concentration is within each of the bins).  Of course for any data point, only a
single bin membership variable will take the value 1, while the rest will be
zero. \\

PLOT COLIN-1\\

This kind of binary variable representation therefore allows us to use a linear
model framework to capture nonlinearities in the real variable.  Nothing is for
free though, and the cost of this flexibility becomes at model training time.
For our new 4-feature binary model, we now have to learn 4 coefficients, whereas
in the single floating point model, we had to learn 2 (one for the concentration
coefficient and one for the bias).  So the utility of this kind of strategy
depends strongly on how much data is available.  \\

Though binning real valued features is a nice trick, it is by no means the only
way binary variables arise.  Almost any membership description (e.g. any
demographic membership) is naturally featurized as a binary variable.  So, if
you'll be doing modeling in any real context, you should expect to be familiar.
As an added bonus, binary valued variables will simplify the action of the
regression equation (CONSISTENT NAME NEEDED HERE).  So, rest of this chapter,
we'll develop another fictitious example to help us explore 
\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

\subsection{Extended Example: Income by City}

Suppose we wanted to understand (and predict) how a person's city of residence
impacts their starting income in their first professional job. To simplify our
work, let's say we're interested in particular job category ("Data Scientist"),
and we're interested in the "residence" effect between the cities of Seattle and
Detroit.\\
\\
%\begin{center}
\begin{table}
\centering
\begin{tabular}{ | l | l | l |}
\hline
\multicolumn{3}{ |c| }{Data Scientist Starting Salaries} \\
\hline
Example Id  & City & Income \\ \hline
000 &  Seattle  & \$107,805 \\ \hline    
001 &  Seattle  & \$107,150 \\ \hline 
002 &  Detroit  & \$106,826 \\ \hline 
003 &  Detroit  & \$ 94,399 \\ \hline 
004 &  Detroit  & \$ 95,589 \\ \hline 
005 &  Seattle  & \$105,981 \\ \hline 
006 &  Detroit  & \$101,601 \\ \hline 
007 &  Seattle  & \$108,543 \\ \hline 
008 &  Seattle  & \$115,971 \\ \hline 
009 &  Detroit  & \$100,728 \\ \hline 

\end{tabular}
\end{table}
\\
\\
%\end{center}

Forgetting about regression for a moment, how might we handle this problem given
this data? A simple solution is to estimate the mean salary for each city and
compare.  Explicitly, this means we'd calculate:\\
\\
Detroit:
\begin{equation}
\label{naive_avg_1} 
\frac{\$106,826 + \$94,399 + \$95,589 + \$101,601 + \$100,728}{5} = \$99,828.60
\end{equation}
Seattle:
\begin{equation}
\label{naive_avg_2}
\frac{\$107,805 + \$107,150 + \$105,981 + \$108,543 + \$115,971}{5} = \$109,090.00
\end{equation}

Then we'd estimate the city-level effect to be
\begin{equation} 
\$109,090.00 - \$99,828.60 = \$9,261.40 
\end{equation}

Now let's take a look at how we would solve this problem in a linear regression
framework, and see how it compares to the simple averaging methodology.

First, we need to encode the city membership in terms of binary x-variables
(features) of the model.  There are two obvious ways to do this. \\
\\
1. Create two symmetric \emph{indicator} variables: one where '1' indicates to
Seattle residence, and a second where '1' indicates Detroit residence.\\
\\
2. Create a single city indicator and a "bias" variable.  The indicator variable
could be the above where '1' indicates Seattle residency and '0' indicates
Detroit.  The difference in the second (bias) variable, which takes the value
'1' for all data points, regardless of residency.\\
\\
Let's start with the first strategy.  In this case, our data is represented like
this:\\
\\
%\begin{center}
\begin{table}
\centering
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Seattle\_indicator & Detroit\_indicator & Income \\ \hline
000 &  1  &  0  & \$107,805 \\ \hline    
001 &  1  &  0  & \$107,150 \\ \hline 
002 &  0  &  1  & \$106,826 \\ \hline 
003 &  0  &  1  & \$ 94,399 \\ \hline 
004 &  0  &  1  & \$ 95,589 \\ \hline 
005 &  1  &  0  & \$105,981 \\ \hline 
006 &  0  &  1  & \$101,601 \\ \hline 
007 &  1  &  0  & \$108,543 \\ \hline 
008 &  1  &  0  & \$115,971 \\ \hline 
009 &  0  &  1  & \$100,728 \\ \hline 

\end{tabular}
\label{income_indicator_rep}
\end{table}
%\end{center}
\\
\\
Now we can express the components in the OLS regression solution formula.  The
design matrix and target variable are:

\[
X = \left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
,
\quad\quad
Y = \left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\] 

Now let's break the OLS equation ($\hat{w} = (X^T X)^{-1} X^T Y $) down
piece-by-piece. \\
\\
First let's examine the product of the design matrix and the target.

\begin{align}
X^T Y &= 
\left(\begin{array}{cccccccccc}
         1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
         0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\\
X^T Y &= 
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\end{align}
\\
If you're noticing similarities to the naive averages in ~\ref{naive_avg_1} and
~\ref{naive_avg_2}, you're on the right track.\\

Now let's look at the product of the design matrix with itself:
\begin{align}
X^T X &=
\left(\begin{array}{cccccccccc}
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
          0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{cc}
          5 & 0 \\
          0 & 5 
\end{array}\right)
\end{align}
\\
Diagonal matrices are notoriously easy to invert, so the factor that explicitly
appears in the OLS equation is:
\begin{equation}
(X^T X)^{-1} =  
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\end{equation}
\\
Putting this all together gives:
\begin{align} 
\hat{w} &= (X^T X)^{-1} X^T Y 
\\
\hat{w} &= 
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\\
\\
\hat{w} &= 
\left(\begin{array}{c}
         \displaystyle\frac{107,805 + 107,150 + 105,981 + 108,543 + 115,971}{5} \\
         \\
         \displaystyle\frac{106,826 +  94,399 +  95,589 + 101,601 + 100,728}{5} 
\end{array}\right)
\\
\hat{w} &= 
\left(\begin{array}{c}
          109,090.00 \\
          99,828.60
\end{array}\right)
\end{align}
\\
The OLS calculation not only give the same answer as the naive averaging, it
encodes the exact logic of that calculation.  The calculation of $X^T Y$ simply
collects the measured incomes into the two groups based on residency and sums
them, while the calculation of $(X^T X)^{-1}$ constructs the multiplier
($1/5$) to take the average for each group. In this case, we had equal sized
groups, so the averaging process requires the same divisor for both groups.
However, had the groups been of differing sizes, the OLS equation would
have gracefully adjusted the denominator as required by the group size.\\
%\\
%For those of you who may remember your early statistics classes, you may have
%seen the the problem of calculating observational means on different
%sub-populations handled by a technique calle ANOVA. The prototype ANOVA
%experiment would be one in which differing treatments ($t_i$) are given to
%sub-populations ($i$), and their relative effects on an output variable ($y$)
%are to be measured....NEEDS MORE\\
\\
As we developed our indicator-variable representation of table 
\ref{income_indicator_rep}, we noted an alternative represention, using a single
binary indicator variable (Seattle-hood), and a "binary" bias variable, which
was identically '1' for all data examples.   








