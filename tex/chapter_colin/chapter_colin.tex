\section{Real Models and Real Problems: Feature Colinearity}

\subsection{Introduction: "Real" vs Binary Model Features}

We've worked through the matrix algebra in an abstract mathematical setting to
show the fundamental equation that drives ordinary least squares regression:

\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

It might be tempting at this point to accept this equation and move on,
considering it just a tool for future use.  We propose another perspective: that
this equation is a \emph{starting} point for building our intuitions and
deepening our understanding; that it's a distilled essence that has much to
teach us. \\

It will be easier to dive into this equation if we consider a new type of
model feature (or x-variable): binary-valued features.  So far, we've worked
exclusively with x-variables that can take on arbitrary floating point (real)
values.  Such variables are important and have wide application, but equally
important are binary features--x-variables that are allowed to take on only two
values, 0 or 1.\\

At first, binary features may seem constritive compared to real features, but in
fact they can allow our models to become quite a bit \emph{more} flexible.
Consider the following (fictional) example of algae mass as a function of
concentration of the algae-unfriendsly chemical X.  As the concentration rises,
algae mass decreases to zero, but the rate of decrease slows as the
concentration increases, and of course the algae mass cannot be negative.  A
linear fit using the real value of the concentration fails on both of these
counts, and would clearly not generalize well to high concentrations.\\

Another option is to "bin" concentration values (less than 1, 1-2, 2-5, 5-10,
more than 10), and establish binary variables corresponding to each bin.  So
instead of having a single decimal number associated with it, each data point
will have now 4 binary numbers associated with it (indicating whether or not the
concentration is within each of the bins).  Of course for any data point, only a
single bin membership variable will take the value 1, while the rest will be
zero. \\

PLOT COLIN-1\\

This kind of binary variable representation therefore allows us to use a linear
model framework to capture nonlinearities in the real variable.  Nothing is for
free though, and the cost of this flexibility becomes at model training time.
For our new 4-feature binary model, we now have to learn 4 coefficients, whereas
in the single floating point model, we had to learn 2 (one for the concentration
coefficient and one for the bias).  So the utility of this kind of strategy
depends strongly on how much data is available.  \\

Though binning real valued features is a nice trick, it is by no means the only
way binary variables arise.  Almost any membership description (e.g. any
demographic membership) is naturally featurized as a binary variable.  So, if
you'll be doing modeling in any real context, you should expect to be familiar.
As an added bonus, binary valued variables will simplify the action of the
regression equation (CONSISTENT NAME NEEDED HERE).  So, rest of this chapter,
we'll develop another fictitious example to help us explore 
\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

\subsection{Extended Example: Income by City}

Suppose we wanted to understand (and predict) how a person's city of residence
impacts their starting income in their first professional job. To simplify our
work, let's say we're interested in particular job category ("Data Scientist"),
and we're interested in the "residence" effect between the cities of Seattle and
Detroit.\\
\\
%\begin{center}
\begin{tabular}{ | l | l | l |}
\hline
\multicolumn{3}{ |c| }{Data Scientist Starting Salaries} \\
\hline
Example Id  & City & Income \\ \hline
000 &  Seattle  & \$107,805 \\ \hline    
001 &  Seattle  & \$107,150 \\ \hline 
002 &  Detroit  & \$106,826 \\ \hline 
003 &  Detroit  & \$ 94,399 \\ \hline 
004 &  Detroit  & \$ 95,589 \\ \hline 
005 &  Seattle  & \$105,981 \\ \hline 
006 &  Detroit  & \$101,601 \\ \hline 
007 &  Seattle  & \$108,543 \\ \hline 
008 &  Seattle  & \$115,971 \\ \hline 
009 &  Detroit  & \$100,728 \\ \hline 

\end{tabular}
\\
\\
%\end{center}

Forgetting about regression for a moment, how might we handle this problem given
this data? A simple solution is to estimate the mean salary for each city and
compare.  Explicitly, this means we'd calculate:\\
\\
Detroit:
\begin{equation} 
\frac{\$106,826 + \$94,399 + \$95,589 + \$101,601 + \$100,728}{5} = \$99,828.60
\end{equation}
Seattle:
\begin{equation} 
\frac{\$107,805 + \$107,150 + \$105,981 + \$108,543 + \$115,971}{5} = \$109,090.00
\end{equation}

Then we'd estimate the city-level effect to be
\begin{equation} 
\$109,090.00 - \$99,828.60 = \$9,261.40 
\end{equation}

Now let's take a look at how we would solve this problem in a linear regression
framework, and see how it compares to the simple averaging methodology.

First, we need to encode the city membership in terms of binary x-variables
(features) of the model.  There are two obvious ways to do this. \\
\\
1. Create two symmetric variables: one where '1' indicates to Seattle residence,
and a second where '1' indicates Detroit residence.\\
\\
2. Create a single city indicator and a "bias" variable.  For example, a variable
like the above which is '1' for Seattle residents and '0' for Detroit.  The
second (bias) variable takes the value '1' for all data points, regardless of
their residency.\\
\\
Let's start with the first strategy.  In this case, our data is represented like
this:\\
\\
%\begin{center}
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Seattle\_indicator & Detroit\_indicator & Income \\ \hline
000 &  1  &  0  & \$107,805 \\ \hline    
001 &  1  &  0  & \$107,150 \\ \hline 
002 &  0  &  1  & \$106,826 \\ \hline 
003 &  0  &  1  & \$ 94,399 \\ \hline 
004 &  0  &  1  & \$ 95,589 \\ \hline 
005 &  1  &  0  & \$105,981 \\ \hline 
006 &  0  &  1  & \$101,601 \\ \hline 
007 &  1  &  0  & \$108,543 \\ \hline 
008 &  1  &  0  & \$115,971 \\ \hline 
009 &  0  &  1  & \$100,728 \\ \hline 

\end{tabular}
%\end{center}
\\
\\
Now we can express the components in the OLS regression solution formula.  The
design matrix and target variable are:

\[
X = \left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
,
\quad\quad
Y = \left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\] 

Let's break the OLS equation ($\hat{w} = (X^T X)^{-1} X^T Y $) down
piece-by-piece:

\[
Y = \left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\] 




