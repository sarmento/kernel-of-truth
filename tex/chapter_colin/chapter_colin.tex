\section{Real Models and Real Problems: Feature Colinearity}

\subsection{Introduction: "Real" vs Binary Model Features}

We've worked through the matrix algebra in an abstract mathematical setting to
show the fundamental equation that drives ordinary least squares regression:

\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

It might be tempting at this point to accept this equation and move on,
considering it just a tool for future use.  We propose another perspective: that
this equation is a \emph{starting} point for building our intuitions and
deepening our understanding; that it's a distilled essence that has much to
teach us. \\

It will be easier to dive into this equation if we consider a new type of
model feature (or x-variable): binary-valued features.  So far, we've worked
exclusively with x-variables that can take on arbitrary floating point (real)
values.  Such variables are important and have wide application, but equally
important are binary features--x-variables that are allowed to take on only two
values, 0 or 1.\\

At first, binary features may seem constritive compared to real features, but in
fact they can allow our models to become quite a bit \emph{more} flexible.
Consider the following (fictional) example of algae mass as a function of
concentration of the algae-unfriendsly chemical X.  As the concentration rises,
algae mass decreases to zero, but the rate of decrease slows as the
concentration increases, and of course the algae mass cannot be negative.  A
linear fit using the real value of the concentration fails on both of these
counts, and would clearly not generalize well to high concentrations.\\

Another option is to "bin" concentration values (less than 1, 1-2, 2-5, 5-10,
more than 10), and establish binary variables corresponding to each bin.  So
instead of having a single decimal number associated with it, each data point
will have now 4 binary numbers associated with it (indicating whether or not the
concentration is within each of the bins).  Of course for any data point, only a
single bin membership variable will take the value 1, while the rest will be
zero. \\

PLOT COLIN-1\\

This kind of binary variable representation therefore allows us to use a linear
model framework to capture nonlinearities in the real variable.  Nothing is for
free though, and the cost of this flexibility becomes at model training time.
For our new 4-feature binary model, we now have to learn 4 coefficients, whereas
in the single floating point model, we had to learn 2 (one for the concentration
coefficient and one for the bias).  So the utility of this kind of strategy
depends strongly on how much data is available.  \\

Though binning real valued features is a nice trick, it is by no means the only
way binary variables arise.  Almost any membership description (e.g. any
demographic membership) is naturally featurized as a binary variable.  So, if
you'll be doing modeling in any real context, you should expect to be familiar.
As an added bonus, binary valued variables will simplify the action of the
regression equation (CONSISTENT NAME NEEDED HERE).  So, rest of this chapter,
we'll develop another fictitious example to help us explore 
\begin{equation} \hat{w} = (X^T X)^{-1} X^T Y \end{equation}

\subsection{Extended Example: Income by City}

Suppose we wanted to understand (and predict) how a person's city of residence
impacts their starting income in their first professional job. To simplify our
work, let's say we're interested in particular job category ("Data Scientist"),
and we're interested in the "residence" effect between the cities of Seattle and
Detroit.\\
\\
%\begin{center}
\begin{table}
\centering
\begin{tabular}{ | l | l | l |}
\hline
\multicolumn{3}{ |c| }{Data Scientist Starting Salaries} \\
\hline
Example Id  & City & Income \\ \hline
000 &  Seattle  & \$107,805 \\ \hline    
001 &  Seattle  & \$107,150 \\ \hline 
002 &  Detroit  & \$106,826 \\ \hline 
003 &  Detroit  & \$ 94,399 \\ \hline 
004 &  Detroit  & \$ 95,589 \\ \hline 
005 &  Seattle  & \$105,981 \\ \hline 
006 &  Detroit  & \$101,601 \\ \hline 
007 &  Seattle  & \$108,543 \\ \hline 
008 &  Seattle  & \$115,971 \\ \hline 
009 &  Detroit  & \$100,728 \\ \hline 

\end{tabular}
\end{table}
\\
\\
%\end{center}

Forgetting about regression for a moment, how might we handle this problem given
this data? A simple solution is to estimate the mean salary for each city and
compare.  Explicitly, this means we'd calculate:\\
\\
Detroit:
\begin{equation}
\label{naive_avg_1} 
\frac{\$106,826 + \$94,399 + \$95,589 + \$101,601 + \$100,728}{5} = \$99,828.60
\end{equation}
Seattle:
\begin{equation}
\label{naive_avg_2}
\frac{\$107,805 + \$107,150 + \$105,981 + \$108,543 + \$115,971}{5} = \$109,090.00
\end{equation}

Then we'd estimate the city-level effect to be
\begin{equation} 
\$109,090.00 - \$99,828.60 = \$9,261.40 
\end{equation}

Now let's take a look at how we would solve this problem in a linear regression
framework, and see how it compares to the simple averaging methodology.

First, we need to encode the city membership in terms of binary x-variables
(features) of the model.  There are two obvious ways to do this. \\
\\
1. Create two symmetric \emph{indicator} variables: one where '1' indicates to
Seattle residence, and a second where '1' indicates Detroit residence.\\
\\
2. Create a single city indicator and a "bias" variable.  The indicator variable
could be the above where '1' indicates Seattle residency and '0' indicates
Detroit.  The difference in the second (bias) variable, which takes the value
'1' for all data points, regardless of residency.\\
\\
Let's start with the first strategy.  In this case, our data is represted as
shown in table \ref{income_indicator_rep}\\

\begin{table}
\label{income_indicator_rep}
\centering
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Seattle\_indicator & Detroit\_indicator & Income \\ \hline
000 &  1  &  0  & \$107,805 \\ \hline    
001 &  1  &  0  & \$107,150 \\ \hline 
002 &  0  &  1  & \$106,826 \\ \hline 
003 &  0  &  1  & \$ 94,399 \\ \hline 
004 &  0  &  1  & \$ 95,589 \\ \hline 
005 &  1  &  0  & \$105,981 \\ \hline 
006 &  0  &  1  & \$101,601 \\ \hline 
007 &  1  &  0  & \$108,543 \\ \hline 
008 &  1  &  0  & \$115,971 \\ \hline 
009 &  0  &  1  & \$100,728 \\ \hline 

\end{tabular}
\caption{Income per person by city of residence data, represented in terms of symmetric
indicator variables (one for each city).}  
\end{table}

Now we can express the components in the OLS regression solution formula.  The
design matrix and target variable are:

\[
X = \left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
,
\quad\quad
Y = \left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\] 

Now let's break the OLS equation ($\hat{w} = (X^T X)^{-1} X^T Y $) down
piece-by-piece. \\
\\
First let's examine the product of the design matrix and the target.

\begin{align}
X^T Y &= 
\left(\begin{array}{cccccccccc}
         1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
         0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\\
X^T Y &= 
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\end{align}
\\
If you're noticing similarities to the naive averages in ~\ref{naive_avg_1} and
~\ref{naive_avg_2}, you're on the right track.\\

Now let's look at the product of the design matrix with itself:
\begin{align}
X^T X &=
\left(\begin{array}{cccccccccc}
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
          0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
\end{array}\right)
\cdot
\left(\begin{array}{cc}
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          0  &  1 \\ 
          1  &  0 \\ 
          1  &  0 \\ 
          0  &  1
\end{array}\right)
\\
X^T X &=
\left(\begin{array}{cc}
          5 & 0 \\
          0 & 5 
\end{array}\right)
\end{align}
\\
Diagonal matrices are notoriously easy to invert, so the factor that explicitly
appears in the OLS equation is:
\begin{equation}
(X^T X)^{-1} =  
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\end{equation}
\\
Putting this all together gives:
\begin{align} 
\hat{w} &= (X^T X)^{-1} X^T Y 
\\
\hat{w} &= 
\left(\begin{array}{cc}
          \frac{1}{5} & 0 \\
          0           & \frac{1}{5} 
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 \\
         106,826 +  94,399 +  95,589 + 101,601 + 100,728 
\end{array}\right)
\\
\\
\hat{w} &= 
\left(\begin{array}{c}
         \displaystyle\frac{107,805 + 107,150 + 105,981 + 108,543 + 115,971}{5} \\
         \\
         \displaystyle\frac{106,826 +  94,399 +  95,589 + 101,601 + 100,728}{5} 
\end{array}\right)
\\
\hat{w} &= 
\left(\begin{array}{c}
          109,090.00 \\
          99,828.60
\end{array}\right)
\end{align}
\\
The OLS calculation not only give the same answer as the naive averaging, it
encodes the exact logic of that calculation.  The calculation of $X^T Y$ simply
collects the measured incomes into the two groups based on residency and sums
them, while the calculation of $X^T X$ counts examples in each class, making
$(X^T X)^{-1}$ into the multipliers ($1/5$) to calculate the means. In this
case, we had equal sized groups, so the averaging process requires the same
divisor for both groups.  However, had the groups been of differing sizes, the
OLS equation would have gracefully adjusted the denominator as required by the
group size.\\
\\
To generalize (and develop some notation)
%\\
%For those of you who may remember your early statistics classes, you may have
%seen the the problem of calculating observational means on different
%sub-populations handled by a technique calle ANOVA. The prototype ANOVA
%experiment would be one in which differing treatments ($t_i$) are given to
%sub-populations ($i$), and their relative effects on an output variable ($y$)
%are to be measured....NEEDS MORE\\
\\
When we developed our indicator-variable representation of table
\ref{income_indicator_rep} \{REFERENCE WRONG\}, we noted that there was an
alternative represention, using a single binary indicator variable
(Seattle-hood), and a "binary" bias variable, which was identically '1' for all
data examples.  We show this representation in table \ref{bias_residence_rep}. 

\begin{table}
\label{bias_residence_rep}
\centering
\begin{tabular}{ | l | l | l || c |}
\hline
\multicolumn{4}{ |c| }{Data Scientist Starting Salaries -- Inidicator Variable Encoding} \\
\hline
Example Id & Bias & Residence & Income \\ \hline
000 &  1  &  1  & \$107,805 \\ \hline    
001 &  1  &  1  & \$107,150 \\ \hline 
002 &  1  &  0  & \$106,826 \\ \hline 
003 &  1  &  0  & \$ 94,399 \\ \hline 
004 &  1  &  0  & \$ 95,589 \\ \hline 
005 &  1  &  1  & \$105,981 \\ \hline 
006 &  1  &  0  & \$101,601 \\ \hline 
007 &  1  &  1  & \$108,543 \\ \hline 
008 &  1  &  1  & \$115,971 \\ \hline 
009 &  1  &  0  & \$100,728 \\ \hline 
\end{tabular}
\caption{Income per person by city of residence data, represented in terms of
bias and residence variables.}
\end{table}
\begin{equation}
X^T X =
\left(\begin{array}{cccccccccc}
          1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
          1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 
\end{array}\right)
\cdot
\left(\begin{array}{cc}
         1  &  1  \\ 
         1  &  1  \\ 
         1  &  0  \\ 
         1  &  0  \\ 
         1  &  0  \\ 
         1  &  1  \\ 
         1  &  0  \\ 
         1  &  1  \\ 
         1  &  1  \\ 
         1  &  0 
\end{array}\right)
=
\left(\begin{array}{cc}
          10 & 5 \\
          5  & 5 
\end{array}\right)
\end{equation}
\\
Unlike last time, where the diagonal terms counted the membership in each class,
here the diagonal encodes the number of 1's in the bias vector (10) and the
number of data points from the Seattle indicator class (5).  This time, let's
factor the matrix a bit.  There's nothing special about having 5 training
examples in each class, after all. Going forward we'll refer to the total number
of trianing examples as $N_e$.
\\
\begin{equation}
X^T X =
\left(\begin{array}{cc}
          10 & 5 \\
          5  & 5 
\end{array}\right)
= 5
\left(\begin{array}{cc}
          2 & 1 \\
          1 & 1 
\end{array}\right)
= \frac{N_e}{2}
\left(\begin{array}{cc}
          2 & 1 \\
          1 & 1 
\end{array}\right)
\end{equation}
\\
Giving:
\begin{equation}
(X^T X)^{-1} = \frac{2}{N_e}
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\end{equation}

We also have
\begin{align}
X^T Y &= 
\left(\begin{array}{cccccccccc}
         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
         1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
\end{array}\right)
\cdot
\left(\begin{array}{c}
         107,805 \\ 
         107,150 \\ 
         106,826 \\ 
          94,399 \\ 
          95,589 \\ 
         105,981 \\ 
         101,601 \\ 
         108,543 \\ 
         115,971 \\ 
         100,728 
\end{array}\right)
\\
X^T Y &= 
\left(\begin{array}{c}
         107,805 + 107,150 + 106,826 + ... + 108,543 + 115,971 + 100,728 \\
         107,805 + 107,150 + 105,981 + 108,543 + 115,971 
\end{array}\right)
\end{align}
The sum on top is over all data points, while the bottom is the sum over all
Seattle incomes.
We can factor this term similarly to $X^T X$ if we note that the average
over all incomes ($\bar{Y}$) is $\$104,459.3$, and the average over Seattle
incomes ($\bar{Y}_{Sea}$) is
$\$109,090.0$.
\begin{align}
X^T Y &= \frac{N_e}{2} 
\left(\begin{array}{c}
         2 \cdot 104,459.3 \\
         109,090.0 
\end{array}\right)
= 
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\end{align}
Now let's calculate the model coefficients:
\begin{align}
\hat{w} = (X^T X)^{-1} X^T Y
&= \frac{2}{N_e}
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\cdot 
\frac{N_e}{2} 
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\\
&= 
\left(\begin{array}{cc}
          1  & -1 \\
         -1  &  2 
\end{array}\right)
\left(\begin{array}{c}
         2 \cdot \bar{Y} \\
          \bar{Y}_\mathit{Sea} 
\end{array}\right)
\\
&= 
\left(\begin{array}{c}
         2 \bar{Y} -   \bar{Y}_\mathit{Sea}  \\
        -2 \bar{Y} + 2 \bar{Y}_\mathit{Sea} 
\end{array}\right)
\end{align}
These terms may not look interpretable at first, but let's expand them out.
We'll put the incomes from Seattle residents in bold to help clarify what's
happening.
\begin{align}
\hat{w}_\mathit{bias} = 2 \bar{Y} -   \bar{Y}_{Sea} = \frac{1}{5}
  (&\mathbf{107,805} + \mathbf{107,150} + 106,826 + 94,399 + 95,589 \\
  &+ \mathbf{105,981} + 101,601 + \mathbf{108,543} + \mathbf{115,971} + 100,728)\\
- &\frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
\mathbf{108,543} + \mathbf{115,971})\\
= \frac{1}{5}(&106,826 + 94,399 + 95,589 + 101,601 + 100,728) 
= \$99,828.60
\end{align}
In calculating the model's bias coefficient, the matrix mathmatics is just
encoding an average over non-Seattle incomes (in agreement with our previous
results). This makes sense, because when we ask the model to predict an income,
we'll calculate:
\begin{equation}
y = \hat{w}_\mathit{bias} \cdot 1 + \hat{w}_\mathit{Res} \cdot x_\mathit{Res}
\end{equation}
Where $x_\mathit{Res}$ is 1 if the resident is from Seattle, 0 if not. Thie
means for a non-Seattle (Detroit) resident, the prediction is simply
\begin{equation}
y_\mathit{Det} = \hat{w}_\mathit{bias}
\end{equation}
The bias coefficient is the average of all Detroit residents because it is the
sole term in the income prediction for Detroit residents.\\
\\
Let's do the same thing to the second model coefficient, which we'll call
$\hat{w}_\mathit{Res}$. (Why not call this $\hat{w}_\mathit{Sea}$? Hang on and
you'll see.)
\begin{align}
\hat{w}_\mathit{Res} 
    &= -2 \bar{Y} + 2 \bar{Y}_\mathit{Sea} \\
    &= -\frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + 106,826 + 94,399 + 95,589 \\
    &\qquad + \mathbf{105,981} + 101,601 + \mathbf{108,543} + \mathbf{115,971} + 100,728)\\
    &\quad + \frac{2}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
                   \mathbf{108,543} + \mathbf{115,971})\\
    &= \frac{1}{5}(\mathbf{107,805} + \mathbf{107,150} + \mathbf{105,981} +
                   \mathbf{108,543} + \mathbf{115,971})\\
    &\quad -\frac{1}{5}(106,826 + 94,399 + 95,589 + 101,601 + 100,728) 
    &= \$9,261.39
\end{align}
The "residence" model coefficient is the difference between the average incomes
in the two cities (or, equivalently, the average difference in income).  This
makes sense in terms of model prediction, given the meaning of the bias
coefficient.  For a Seattle resident, the model predicts
\begin{align}
y &= \hat{w}_\mathit{bias} \cdot 1 + \hat{w}_\mathit{Res} \cdot x_\mathit{Res}\\
y &= \hat{w}_\mathit{bias} + \hat{w}_\mathit{Res}\\
y &= \$99,828.60 + \$9,261.39 = \$109,090.00 
\end{align}
With the indicator variable encoding, the model weights represented mean values
for each rewidency group, but for the bias/residency variable encoding, the
model weights represent a mean for one of the groups (Detroit) and a
differential effect size (the income gain one might expect when moving to
Seattle from Detroit). Note that this doesn't affect the outcome, both models
predict the same value for Seattle/Detroit resident incomes.  However, it's
now clear why we cautioned against naming the second model coefficient
$\hat{w}_\mathit{Sea}$.  This name could mislead us into thinking the model
weight is the mean of the Seattle group, when in fact it's a measure of the
emph{difference} between the groups. \\

\subsection{More features, more problems}








